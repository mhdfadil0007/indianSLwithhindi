# ğŸ¤Ÿ AI Sign Language Recognition & Translation Platform  
### Full-Stack Web Application with Real-Time Gesture Detection

---

# ğŸ“Œ 1. Project Overview

The **AI Sign Language Recognition & Translation Platform** is a full-stack AI-powered web application designed to bridge communication between hearing-impaired individuals and non-sign users.

The system integrates:

- Natural Language Processing (NLP)
- Computer Vision (MediaPipe)
- Machine Learning (SVM Classifier)
- Real-Time Web Integration (Django + JavaScript)

The platform provides two major modules:

1ï¸âƒ£ **Text â†’ Sign Language Animation**  
2ï¸âƒ£ **Live Sign Language â†’ Text Detection (Webcam-Based)**  

---

# ğŸ¯ 2. Objectives

- Convert English and Hindi text into animated sign language
- Detect live hand gestures using webcam
- Classify static alphabets (Aâ€“Z)
- Recognize dynamic word gestures
- Integrate AI directly into a real-time web application
- Provide secure login-based access

---

# ğŸ— 3. Complete System Architecture

```
Frontend (HTML + CSS + JavaScript)
            â†“
Django Backend (Python)
            â†“
MediaPipe (Hand Landmark Detection)
            â†“
Feature Extraction (63 Features)
            â†“
SVM Machine Learning Model
            â†“
Real-Time Prediction Output
```

indianSLwithhindi/
â”‚
â”œâ”€â”€ manage.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ db.sqlite3
â”‚
â”œâ”€â”€ A2SL/ â†’ Main Django Project Folder
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ settings.py
â”‚ â”œâ”€â”€ urls.py
â”‚ â”œâ”€â”€ wsgi.py
â”‚ â”œâ”€â”€ asgi.py
â”‚ â””â”€â”€ views.py
â”‚
â”œâ”€â”€ templates/ â†’ HTML Files
â”‚ â”œâ”€â”€ base.html
â”‚ â”œâ”€â”€ index.html
â”‚ â””â”€â”€ animation.html
â”‚
â”œâ”€â”€ static/ â†’ CSS / JS (if used)
â”‚ â”œâ”€â”€ css/
â”‚ â””â”€â”€ js/
â”‚
â”œâ”€â”€ assets/ â†’ Sign Language Videos
â”‚ â”œâ”€â”€ A.mp4
â”‚ â”œâ”€â”€ B.mp4
â”‚ â”œâ”€â”€ Hello.mp4
â”‚ â”œâ”€â”€ Thank.mp4
â”‚ â””â”€â”€ ...
â”‚
â”œâ”€â”€ media/ â†’ (Optional uploads)
â”‚
â””â”€â”€ env/ â†’ Virtual Environment

---

# ğŸ”µ MODULE 1 â€” Text to Sign Animation

## ğŸ”„ How It Works (Step-by-Step Flow)

```
User Input
     â†“
Django View (POST request)
     â†“
Language Check
     â†“
Translation (if Hindi)
     â†“
Tokenization (NLTK)
     â†“
Lemmatization
     â†“
Stopword Filtering
     â†“
Check if word.mp4 exists
     â†“
Send list of videos to frontend
     â†“
JavaScript plays animations sequentially
```

---

## ğŸŒ Translation Logic

If the user selects Hindi:

```python
translated_text = translator.translate(text, src="hi", dest="en").text.lower()
```

Symbolically:

```
Hindi Text
    â†“
Google Translator API
    â†“
English Text
```

Why translate to English?

Because animation videos are stored as:

```
how.mp4
are.mp4
you.mp4
```

So all input must be normalized into English for matching.

---

## ğŸ§  NLP Processing

Libraries Used:
- nltk
- googletrans
- gTTS

Processing includes:
- Tokenization
- POS Tagging
- Custom Lemmatization
- Stopword Filtering
- Protected verb preservation (e.g., "are" is not converted to "be")

Example:

Input:
```
How are you?
```

Processed:
```
["how", "are", "you"]
```

---

# ğŸŸ¢ MODULE 2 â€” Live Sign Detection (AI Module)

## ğŸ”„ Live Detection Flow

```
Webcam Frame
     â†“
JavaScript captures image
     â†“
Convert image â†’ Base64
     â†“
Send to Django (/receive-frame/)
     â†“
Decode image (OpenCV)
     â†“
MediaPipe detects hand
     â†“
Extract 21 landmarks
     â†“
Convert to 63 numerical features
     â†“
SVM model predicts letter
     â†“
Return JSON response
     â†“
Browser updates UI
```

---

# ğŸ§  Feature Engineering

MediaPipe provides:

- 21 hand landmarks  
- Each landmark has (x, y, z)

So:

```
21 Ã— 3 = 63 features
```

Instead of using raw image pixels (150,000+ values), we use 63 structured geometric features.

This makes the model:
- Faster
- Lightweight
- More efficient
- Easier to train

---

# ğŸ¤– 4. Machine Learning Implementation

## ğŸ”¹ Static Alphabet Model

- 63 landmark features
- SVM (RBF kernel)
- StandardScaler preprocessing
- Label encoding
- 80/20 train-test split

### Why SVM?

- Works well for small datasets
- Good for structured numerical data
- Efficient for real-time prediction
- High accuracy for static gestures

---

## ğŸ”¹ Word Detection Model

- 12 frames per sample
- 126 features per frame (two hands)
- 1512 total features
- Multi-class SVM classifier

Why multiple frames?

Because words involve motion. A single frame cannot capture movement.

---

# ğŸ“Š 5. Dataset Creation

Custom dataset recorders were built.

## Static Alphabet Dataset
- 30 samples per letter
- Total â‰ˆ 780 samples
- Saved as X.npy and y.npy

## Word Dataset
- 20 samples per word
- Multi-frame capture
- 1512 features per sample

---


# ğŸ›  7. Technologies Used

## Backend
- Python
- Django
- OpenCV
- MediaPipe
- Scikit-learn
- NLTK
- Google Translate API
- gTTS

## Frontend
- HTML
- CSS
- JavaScript

## ML
- SVM (Support Vector Machine)
- StandardScaler
- LabelEncoder

---

# ğŸ“ˆ 8. Accuracy

- Alphabet Model: ~90â€“95%
- Word Model: ~75â€“85%

Accuracy depends on:
- Lighting
- Hand stability
- Dataset quality
- Background noise

---

# â“ Frequently Asked Questions (Doubt Clarification Section)

### Q1: Why not use Deep Learning (CNN)?

Deep learning requires:
- Large dataset
- GPU training
- More computation

Since we use structured landmark features (63 values), classical ML like SVM performs efficiently.

---

### Q2: Why not use raw images?

Raw images have thousands of features.

Landmark-based features:
- Reduce dimensionality
- Improve speed
- Improve generalization

---

### Q3: Why StandardScaler?

SVM is distance-based.
Feature scaling ensures all 63 features contribute equally.

---

### Q4: Why multiple frames for words?

Words involve motion.
Static frame cannot capture temporal patterns.

---

### Q5: What are the limitations?

- Sensitive to lighting
- May vary across users
- Not yet sentence-level dynamic detection
- Requires stable hand positioning

---

### Q6: How can this be improved?

- CNN + LSTM deep learning
- Larger dataset
- Multi-user training
- Majority voting for stability
- Cloud deployment

---

# ğŸ” 9. Authentication & Security

- Django login system
- Protected routes
- Session-based authentication

---

# âš™ 10. Installation & Setup

## Step 1: Clone Repositor
```bash
git clone <repository-link>
cd project-folder
```

## Step 2: Create Virtual Environment

```bash
python -m venv tf-env
tf-env\Scripts\activate
```

## Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

## Step 4: Run Server

```bash
python manage.py runserver
```

Access at:

```
http://127.0.0.1:8000
```

---

# ğŸ¥ 11. How to Use

## Text to Sign
1. Login
2. Go to Express
3. Enter sentence
4. Click submit
5. Watch animation

## Live Detection
1. Login
2. Go to Live Detect
3. Start camera
4. Show alphabet sign
5. View real-time prediction

---

# ğŸ“Œ 12. Project Status

- Text-to-Sign Module â€” Completed
- Live Alphabet Detection â€” Completed
- Word Detection â€” Integrated (Standalone + Training)
- UI Optimization â€” Completed
- Future Enhancements â€” Planned

---

# ğŸ† Final Outcome

This project demonstrates:

- Full-stack AI integration
- Real-time computer vision
- NLP-based text processing
- Structured machine learning pipeline
- Modular architecture

It serves as a strong academic AI project and a scalable prototype for real-world assistive technology.

---

# ğŸ“ License

Academic Project â€“ Educational Use Only

---

# ğŸ“ Developed As

Final Year AI-Based Communication System Project




# Changes i made
i changed the versions in requirements.txt to match my python version of 3.12.3
- changed in numpy,opencv,mediapipe,protobuf,tensorflow
- also added scikit-learn in requirements.txt
- i trained the model first by increasing the samples_per_letter to 50 from 30, reduced min_detection_confidence to 0.5 from 0.7, and reduced time.sleep from 0.4 to 0.2(in record_alphabet_static.py folder)
- but it was only showing number 25 in the detected sign instead of a value, even if new signs were shown(in live detection)
- this was because the value was not been converted to alphabet, and the number was straight introduced. and 25 was the letter 'z'
- even when i fixed it, the accuracy was off
- the model was broken, only 3.85 percent accuracy on training data itself
- so i retrained the model again, but the accuracy only pumped to 40 percent
- so i tried data augmentation(which means to add horizontally flipped versions of each sample) and also added probabiity=True for proper confidence scores(on train_alphabets_static.py)
- this increased the accuracy to 90 percent, but still some letters were not accurate
- so i added feature extraction to it and retrained the data again